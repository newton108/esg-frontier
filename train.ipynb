{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "from spacy.tokens import DocBin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_tweets_to_pd(jsonfile):\n",
    "    '''\n",
    "    read paginated json file to df\n",
    "    input (string): .json file created by searchtweets module\n",
    "    output (pd.DataFrame)\n",
    "    '''\n",
    "    raw_df = pd.read_json(jsonfile, lines=True)\n",
    "    df = pd.DataFrame()\n",
    "    for page in raw_df['data']:\n",
    "        df = pd.concat([df, pd.DataFrame(page)])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 15464 entries, 0 to 499\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    15464 non-null  string\n",
      "dtypes: string(1)\n",
      "memory usage: 241.6 KB\n"
     ]
    }
   ],
   "source": [
    "df_positive = json_tweets_to_pd('get_tweets/tweets_positive.json')\n",
    "\n",
    "# drop tweets withheld by the twitter accounts\n",
    "df_positive = df_positive.drop(df_positive[df_positive.withheld.notnull()].index)\n",
    "\n",
    "df_positive.drop(columns=['id', 'withheld'], inplace=True)\n",
    "df_positive = df_positive.astype({'text':'string'})\n",
    "df_positive.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 186204 entries, 0 to 186203\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    186204 non-null  string\n",
      "dtypes: string(1)\n",
      "memory usage: 1.4 MB\n"
     ]
    }
   ],
   "source": [
    "df_tweets = pd.read_csv('data/tweets_dow.csv', usecols = ['text'], dtype={'text':'string'})\n",
    "df_tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_positive = len(df_positive)\n",
    "df_positive['climate_related'] = np.ones(n_positive, dtype=bool)\n",
    "\n",
    "df_negative = df_tweets.sample(n_positive, random_state = 2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keywords = [\"climate action\", \"environment\", \"climate change\", \"renewable\", \"recycle\", \"earth\", \"emission\", \"carbon footprint\", ...]\n",
    "with open(\"keywords.txt\", \"r\") as f:\n",
    "    keywords = [line.rstrip() for line in f]\n",
    "negative = df_negative.text.apply(lambda text: not any([keyword in text.lower() for keyword in keywords]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_negative = df_negative[negative]\n",
    "df_negative['climate_related'] = np.zeros(len(df_negative), dtype=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_positive, df_negative]).sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up(tweet): \n",
    "\n",
    "    parsed_tweet = tweet\n",
    "    \n",
    "    # remove url\n",
    "    parsed_tweet = re.sub(r\"\\S*https?:\\S*\", \"\", parsed_tweet, flags=re.MULTILINE)\n",
    "\n",
    "    # remove non-ascii\n",
    "    parsed_tweet = re.sub(r\"[^\\x00-\\x7F]+\", \"\", parsed_tweet)\n",
    "\n",
    "    # remove 'RT'\n",
    "    parsed_tweet = re.sub(r\"^RT\\s\", \"\", parsed_tweet)\n",
    "    return parsed_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df, embed):\n",
    "    '''\n",
    "    Preprocess the dataframe into spacy pipeline for later classification\n",
    "    ---\n",
    "    Input:\n",
    "    df (DataFrame): Pandas dataframe containing the raw text and outputs.\n",
    "    embed (str): Name of pipeline embedding used\n",
    "\n",
    "    Output:\n",
    "    df (DataFrame): Preprocessed input dataframe\n",
    "    docs (doc): SpaCy doc object that stores text data along with classification\n",
    "    '''\n",
    "\n",
    "    # clean up tweets\n",
    "    df.text = df.text.apply(clean_up)\n",
    "\n",
    "    # Store the data into tuples\n",
    "    data = tuple(zip(df.text.tolist(), df.climate_related.tolist())) \n",
    "    \n",
    "    # Load English library from SpaCy\n",
    "    nlp = spacy.load(embed)\n",
    "    print(data[0])\n",
    "\n",
    "    # Storage for docs\n",
    "    docs = []\n",
    "\n",
    "    # One-hot encoding for the classifications\n",
    "    for doc, label in tqdm(nlp.pipe(data, as_tuples=True), total = len(data)):\n",
    "        \n",
    "        if label:\n",
    "            doc.cats['climate_related'] = 1\n",
    "        else:\n",
    "            doc.cats['climate_related'] = 0\n",
    "        # print(doc.cats)\n",
    "        \n",
    "        docs.append(doc)\n",
    "    return df, docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 24430 Valid: 6108\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, valid_df = train_test_split(df, test_size=0.2)\n",
    "\n",
    "print(\"Train:\",len(train_df), \"Valid:\", len(valid_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the model in installed in current conda env\n",
    "\n",
    "# embed = \"en_core_web_sm\"\n",
    "# embed = \"en_core_web_md\"\n",
    "embed = \"en_core_web_lg\"\n",
    "# embed = \"en_core_web_trf\"\n",
    "\n",
    "assert spacy.util.is_package(embed), embed + \" is not installed. To install, run: `!python -m spacy download \\\"\" + embed + \"\\\"`\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('.@ChuckRobbins shares how we leverage tech to deliver business outcomes for our partners and customers. #CiscoPS16 ', False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24430/24430 [01:13<00:00, 330.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Shes back in all-new adventure in Los Angeles, where the sun shines bright and dreams are forever. Check out this new poster for Hollywood Stargirl!  ', False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6108/6108 [00:22<00:00, 265.77it/s]\n"
     ]
    }
   ],
   "source": [
    "# Covert the train and valid dataframes to .spacy files for training\n",
    "\n",
    "# Preprocess the dataframes for train data\n",
    "train_data, train_docs = preprocess(train_df, embed)\n",
    "# # Save data and docs in a binary file to disc\n",
    "doc_bin = DocBin(docs=train_docs)\n",
    "doc_bin.to_disk(\"./spacy_data/textcat_train.spacy\")\n",
    "\n",
    "# # Preprocess the dataframes for validation data\n",
    "valid_data, valid_docs = preprocess(valid_df, embed)\n",
    "# # Save data and docs in a binary file to disc\n",
    "doc_bin = DocBin(docs=valid_docs)\n",
    "doc_bin.to_disk(\"./spacy_data/textcat_valid.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Generated config template specific for your use case\u001b[0m\n",
      "- Language: en\n",
      "- Pipeline: textcat_multilabel\n",
      "- Optimize for: efficiency\n",
      "- Hardware: CPU\n",
      "- Transformer: None\n",
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
      "config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy init config \"./config.cfg\" --lang en --pipeline textcat_multilabel --optimize efficiency --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-06-19 12:12:31,882] [DEBUG] Config overrides from CLI: ['paths.train', 'paths.dev']\n",
      "\u001b[38;5;4mℹ Saving to output directory: output\u001b[0m\n",
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[2022-06-19 12:12:32,292] [INFO] Set up nlp object from config\n",
      "[2022-06-19 12:12:32,302] [DEBUG] Loading corpus from path: spacy_data/textcat_valid.spacy\n",
      "[2022-06-19 12:12:32,304] [DEBUG] Loading corpus from path: spacy_data/textcat_train.spacy\n",
      "[2022-06-19 12:12:32,304] [INFO] Pipeline: ['textcat_multilabel']\n",
      "[2022-06-19 12:12:32,307] [INFO] Created vocabulary\n",
      "[2022-06-19 12:12:32,312] [INFO] Finished initializing nlp object\n",
      "[2022-06-19 12:12:45,205] [INFO] Initialized pipeline components: ['textcat_multilabel']\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "[2022-06-19 12:12:45,230] [DEBUG] Loading corpus from path: spacy_data/textcat_valid.spacy\n",
      "[2022-06-19 12:12:45,232] [DEBUG] Loading corpus from path: spacy_data/textcat_train.spacy\n",
      "[2022-06-19 12:12:45,252] [DEBUG] Removed existing output directory: output/model-best\n",
      "[2022-06-19 12:12:45,256] [DEBUG] Removed existing output directory: output/model-last\n",
      "\u001b[38;5;4mℹ Pipeline: ['textcat_multilabel']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TEXTC...  CATS_SCORE  SCORE \n",
      "---  ------  -------------  ----------  ------\n",
      "  0       0           0.25       73.96    0.74\n",
      "  0     200          39.67       93.04    0.93\n",
      "  0     400          27.83       95.35    0.95\n",
      "  0     600          21.23       96.13    0.96\n",
      "  0     800          18.39       97.06    0.97\n",
      "  0    1000          15.73       97.68    0.98\n",
      "  0    1200          13.84       98.03    0.98\n",
      "  0    1400          13.28       98.46    0.98\n",
      "  0    1600          10.95       98.71    0.99\n",
      "  0    1800          10.21       98.98    0.99\n",
      "  0    2000           9.28       99.16    0.99\n",
      "  1    2200           6.16       99.28    0.99\n",
      "  1    2400           5.63       99.40    0.99\n",
      "  1    2600           4.94       99.48    0.99\n",
      "  2    2800           4.03       99.54    1.00\n",
      "  2    3000           3.40       99.58    1.00\n",
      "  2    3200           3.16       99.59    1.00\n",
      "  3    3400           3.03       99.62    1.00\n",
      "  3    3600           2.30       99.66    1.00\n",
      "  3    3800           2.35       99.67    1.00\n",
      "  3    4000           1.93       99.68    1.00\n",
      "  4    4200           1.61       99.68    1.00\n",
      "  4    4400           1.68       99.70    1.00\n",
      "  4    4600           1.46       99.70    1.00\n",
      "  5    4800           1.28       99.71    1.00\n",
      "  5    5000           1.24       99.71    1.00\n",
      "  5    5200           1.12       99.71    1.00\n",
      "  6    5400           1.03       99.72    1.00\n",
      "  6    5600           0.86       99.73    1.00\n",
      "  6    5800           0.85       99.73    1.00\n",
      "  7    6000           0.86       99.73    1.00\n",
      "  7    6200           0.67       99.73    1.00\n",
      "  7    6400           0.66       99.74    1.00\n",
      "  8    6600           0.68       99.74    1.00\n",
      "  8    6800           0.54       99.73    1.00\n",
      "  8    7000           0.52       99.74    1.00\n",
      "  8    7200           0.53       99.74    1.00\n",
      "  9    7400           0.41       99.74    1.00\n",
      "  9    7600           0.44       99.74    1.00\n",
      "  9    7800           0.42       99.74    1.00\n",
      " 10    8000           0.36       99.74    1.00\n",
      " 10    8200           0.33       99.74    1.00\n",
      " 10    8400           0.33       99.74    1.00\n",
      " 11    8600           0.31       99.75    1.00\n",
      " 11    8800           0.26       99.75    1.00\n",
      " 11    9000           0.26       99.74    1.00\n",
      " 12    9200           0.25       99.75    1.00\n",
      " 12    9400           0.20       99.75    1.00\n",
      " 12    9600           0.20       99.75    1.00\n",
      " 12    9800           0.24       99.75    1.00\n",
      " 13   10000           0.17       99.75    1.00\n",
      " 13   10200           0.17       99.75    1.00\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "output/model-last\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train ./config.cfg --output ./output --paths.train ./spacy_data/textcat_train.spacy --paths.dev ./spacy_data/textcat_valid.spacy --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "79fabbbd37ad3894f6cfb0be2f75783fc59e43226a3dc1366dded6fba97273cc"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('esg-frontier')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
