{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "pd.options.display.max_colwidth = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_tweets_to_pd(jsonfile):\n",
    "    '''\n",
    "    read paginated json file to df\n",
    "    input (string): .json file created by searchtweets module\n",
    "    output (pd.DataFrame)\n",
    "    '''\n",
    "    raw_df = pd.read_json(jsonfile, lines=True)\n",
    "    df = pd.DataFrame()\n",
    "    for page in raw_df['data']:\n",
    "        df = pd.concat([df, pd.DataFrame(page)])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_tweets.ipynb         search_stock.yaml        tweets_positive.json\n",
      "merge_tweets.ipynb       search_with_query.yaml   tweets_positive_old.json\n",
      "search_positive.yaml     \u001b[1m\u001b[36mtweets_data\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!ls $PWD/get_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 15464 entries, 0 to 499\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    15464 non-null  string\n",
      "dtypes: string(1)\n",
      "memory usage: 241.6 KB\n"
     ]
    }
   ],
   "source": [
    "df_positive = json_tweets_to_pd('get_tweets/tweets_positive.json')\n",
    "\n",
    "# drop tweets withheld by the twitter accounts\n",
    "df_positive = df_positive.drop(df_positive[df_positive.withheld.notnull()].index)\n",
    "\n",
    "df_positive.drop(columns=['id', 'withheld'], inplace=True)\n",
    "df_positive = df_positive.astype({'text':'string'})\n",
    "df_positive.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 186204 entries, 0 to 186203\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    186204 non-null  string\n",
      "dtypes: string(1)\n",
      "memory usage: 1.4 MB\n"
     ]
    }
   ],
   "source": [
    "df_tweets = pd.read_csv('data/tweets_dow.csv', usecols = ['text'], dtype={'text':'string'})\n",
    "df_tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_positive = len(df_positive)\n",
    "df_positive['climate_related'] = np.ones(n_positive, dtype=bool)\n",
    "\n",
    "df_negative = df_tweets.sample(n_positive, random_state = 2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keywords = [\"climate action\", \"environment\", \"climate change\", \"renewable\", \"recycle\", \"earth\", \"emission\", \"carbon footprint\", ...]\n",
    "with open(\"keywords.txt\", \"r\") as f:\n",
    "    keywords = [line.rstrip() for line in f]\n",
    "negative = df_negative.text.apply(lambda text: not any([keyword in text.lower() for keyword in keywords]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_negative = df_negative[negative]\n",
    "df_negative['climate_related'] = np.zeros(len(df_negative), dtype=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_positive, df_negative]).sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up(tweet): \n",
    "\n",
    "    parsed_tweet = tweet\n",
    "    \n",
    "    # remove url\n",
    "    parsed_tweet = re.sub(r\"\\S*https?:\\S*\", \"\", parsed_tweet, flags=re.MULTILINE)\n",
    "\n",
    "    # remove non-ascii\n",
    "    parsed_tweet = re.sub(r\"[^\\x00-\\x7F]+\", \"\", parsed_tweet)\n",
    "\n",
    "    # remove 'RT'\n",
    "    parsed_tweet = re.sub(r\"^RT\\s\", \"\", parsed_tweet)\n",
    "    return parsed_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df, embed):\n",
    "    '''\n",
    "    Preprocess the dataframe into spacy pipeline for later classification\n",
    "    ---\n",
    "    Input:\n",
    "    df (DataFrame): Pandas dataframe containing the raw text and outputs.\n",
    "    embed (str): Name of pipeline embedding used\n",
    "\n",
    "    Output:\n",
    "    df (DataFrame): Preprocessed input dataframe\n",
    "    docs (doc): SpaCy doc object that stores text data along with classification\n",
    "    '''\n",
    "\n",
    "    # clean up tweets\n",
    "    df.text = df.text.apply(clean_up)\n",
    "\n",
    "    # Store the data into tuples\n",
    "    data = tuple(zip(df.text.tolist(), df.climate_related.tolist())) \n",
    "    \n",
    "    # Load English library from SpaCy\n",
    "    nlp = spacy.load(embed)\n",
    "    print(data[0])\n",
    "\n",
    "    # Storage for docs\n",
    "    docs = []\n",
    "\n",
    "    # One-hot encoding for the classifications\n",
    "    for doc, label in tqdm(nlp.pipe(data, as_tuples=True), total = len(data)):\n",
    "        \n",
    "        if label:\n",
    "            doc.cats['climate_related'] = 1\n",
    "        else:\n",
    "            doc.cats['climate_related'] = 0\n",
    "        # print(doc.cats)\n",
    "        \n",
    "        docs.append(doc)\n",
    "    return df, docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 24430 Valid: 6108 Test: 6108\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2)\n",
    "train_df, valid_df = train_test_split(df, test_size=0.2)\n",
    "\n",
    "print(\"Train:\",len(train_df), \"Valid:\", len(valid_df), \"Test:\", len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Smart home tech can help prevent/detect a fire in your home. Learn how:  #ThinkSafe ', False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24430/24430 [01:13<00:00, 334.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('PODCAST: VP Kathryn Karol shares insight on CEO Jim Umpleby, our policy priorities and the new administration.  ', False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6108/6108 [00:19<00:00, 309.00it/s]\n"
     ]
    }
   ],
   "source": [
    "# Covert the train and valid dataframes to .spacy files for training\n",
    "\n",
    "# need to install one of the models to currently activated conda env by running:\n",
    "# python -m spacy download en_core_web_sm\n",
    "# python -m spacy download en_core_web_md\n",
    "# python -m spacy download en_core_web_lg\n",
    "# python -m spacy download en_core_web_trf\n",
    "\n",
    "\n",
    "\n",
    "# embed = \"en_core_web_sm\" # small\n",
    "# embed = \"en_core_web_md\" # middle\n",
    "embed = \"en_core_web_lg\" # large\n",
    "# embed = \"en_core_web_trf\" # roberta\n",
    "\n",
    "# Preprocess the dataframes for train data\n",
    "train_data, train_docs = preprocess(train_df, embed)\n",
    "# Save data and docs in a binary file to disc\n",
    "doc_bin = DocBin(docs=train_docs)\n",
    "doc_bin.to_disk(\"./spacy_data/textcat_train.spacy\")\n",
    "\n",
    "# Preprocess the dataframes for test data\n",
    "valid_data, valid_docs = preprocess(valid_df, embed)\n",
    "# Save data and docs in a binary file to disc\n",
    "doc_bin = DocBin(docs=valid_docs)\n",
    "doc_bin.to_disk(\"./spacy_data/textcat_valid.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Generated config template specific for your use case\u001b[0m\n",
      "- Language: en\n",
      "- Pipeline: textcat_multilabel\n",
      "- Optimize for: efficiency\n",
      "- Hardware: CPU\n",
      "- Transformer: None\n",
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
      "config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy init config \"./config.cfg\" --lang en --pipeline textcat_multilabel --optimize efficiency --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-06-15 01:49:58,893] [DEBUG] Config overrides from CLI: ['paths.train', 'paths.dev']\n",
      "\u001b[38;5;4mℹ Saving to output directory: output\u001b[0m\n",
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[2022-06-15 01:49:59,624] [INFO] Set up nlp object from config\n",
      "[2022-06-15 01:49:59,632] [DEBUG] Loading corpus from path: spacy_data/textcat_valid.spacy\n",
      "[2022-06-15 01:49:59,633] [DEBUG] Loading corpus from path: spacy_data/textcat_train.spacy\n",
      "[2022-06-15 01:49:59,633] [INFO] Pipeline: ['textcat_multilabel']\n",
      "[2022-06-15 01:49:59,636] [INFO] Created vocabulary\n",
      "[2022-06-15 01:49:59,704] [INFO] Finished initializing nlp object\n",
      "[2022-06-15 01:50:10,308] [INFO] Initialized pipeline components: ['textcat_multilabel']\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "[2022-06-15 01:50:10,318] [DEBUG] Loading corpus from path: spacy_data/textcat_valid.spacy\n",
      "[2022-06-15 01:50:10,318] [DEBUG] Loading corpus from path: spacy_data/textcat_train.spacy\n",
      "[2022-06-15 01:50:10,324] [DEBUG] Removed existing output directory: output/model-best\n",
      "[2022-06-15 01:50:10,327] [DEBUG] Removed existing output directory: output/model-last\n",
      "\u001b[38;5;4mℹ Pipeline: ['textcat_multilabel']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TEXTC...  CATS_SCORE  SCORE \n",
      "---  ------  -------------  ----------  ------\n",
      "  0       0           0.25       60.10    0.60\n",
      "  0     200          40.50       93.09    0.93\n",
      "  0     400          27.89       95.10    0.95\n",
      "  0     600          23.59       96.27    0.96\n",
      "  0     800          19.81       97.31    0.97\n",
      "  0    1000          15.04       97.71    0.98\n",
      "  0    1200          14.10       98.20    0.98\n",
      "  0    1400          12.02       98.45    0.98\n",
      "  0    1600          11.56       98.83    0.99\n",
      "  0    1800           9.63       98.98    0.99\n",
      "  0    2000           9.27       99.25    0.99\n",
      "  1    2200           6.15       99.36    0.99\n",
      "  1    2400           5.31       99.46    0.99\n",
      "  1    2600           5.20       99.56    1.00\n",
      "  2    2800           4.33       99.61    1.00\n",
      "  2    3000           3.27       99.61    1.00\n",
      "  2    3200           3.41       99.68    1.00\n",
      "  3    3400           3.08       99.71    1.00\n",
      "  3    3600           2.25       99.72    1.00\n",
      "  3    3800           2.25       99.74    1.00\n",
      "  3    4000           2.18       99.75    1.00\n",
      "  4    4200           1.65       99.75    1.00\n",
      "  4    4400           1.62       99.77    1.00\n",
      "  4    4600           1.67       99.78    1.00\n",
      "  5    4800           1.28       99.79    1.00\n",
      "  5    5000           1.09       99.79    1.00\n",
      "  5    5200           1.29       99.78    1.00\n",
      "  6    5400           1.10       99.79    1.00\n",
      "  6    5600           0.89       99.79    1.00\n",
      "  6    5800           0.90       99.80    1.00\n",
      "  7    6000           0.88       99.81    1.00\n",
      "  7    6200           0.71       99.80    1.00\n",
      "  7    6400           0.71       99.81    1.00\n",
      "  8    6600           0.66       99.81    1.00\n",
      "  8    6800           0.55       99.81    1.00\n",
      "  8    7000           0.55       99.81    1.00\n",
      "  8    7200           0.56       99.81    1.00\n",
      "  9    7400           0.51       99.82    1.00\n",
      "  9    7600           0.44       99.82    1.00\n",
      "  9    7800           0.43       99.82    1.00\n",
      " 10    8000           0.36       99.82    1.00\n",
      " 10    8200           0.31       99.82    1.00\n",
      " 10    8400           0.36       99.82    1.00\n",
      " 11    8600           0.37       99.82    1.00\n",
      " 11    8800           0.29       99.82    1.00\n",
      " 11    9000           0.28       99.83    1.00\n",
      " 12    9200           0.25       99.82    1.00\n",
      " 12    9400           0.21       99.82    1.00\n",
      " 12    9600           0.21       99.82    1.00\n",
      " 12    9800           0.24       99.82    1.00\n",
      " 13   10000           0.16       99.82    1.00\n",
      " 13   10200           0.17       99.82    1.00\n",
      " 13   10400           0.21       99.82    1.00\n",
      " 14   10600           0.14       99.82    1.00\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "output/model-last\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train ./config.cfg --output ./output --paths.train ./spacy_data/textcat_train.spacy --paths.dev ./spacy_data/textcat_valid.spacy --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "79fabbbd37ad3894f6cfb0be2f75783fc59e43226a3dc1366dded6fba97273cc"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('esg-frontier')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
