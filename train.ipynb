{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "from spacy.tokens import DocBin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_tweets_to_pd(jsonfile):\n",
    "    '''\n",
    "    read paginated json file to df\n",
    "    input (string): .json file created by searchtweets module\n",
    "    output (pd.DataFrame)\n",
    "    '''\n",
    "    raw_df = pd.read_json(jsonfile, lines=True)\n",
    "    df = pd.DataFrame()\n",
    "    for page in raw_df['data']:\n",
    "        df = pd.concat([df, pd.DataFrame(page)])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 15464 entries, 0 to 499\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    15464 non-null  string\n",
      "dtypes: string(1)\n",
      "memory usage: 241.6 KB\n"
     ]
    }
   ],
   "source": [
    "df_positive = json_tweets_to_pd('get_tweets/tweets_positive.json')\n",
    "\n",
    "# drop tweets withheld by the twitter accounts\n",
    "df_positive = df_positive.drop(df_positive[df_positive.withheld.notnull()].index)\n",
    "\n",
    "df_positive.drop(columns=['id', 'withheld'], inplace=True)\n",
    "df_positive = df_positive.astype({'text':'string'})\n",
    "df_positive.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 186204 entries, 0 to 186203\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    186204 non-null  string\n",
      "dtypes: string(1)\n",
      "memory usage: 1.4 MB\n"
     ]
    }
   ],
   "source": [
    "df_tweets = pd.read_csv('data/tweets_dow.csv', usecols = ['text'], dtype={'text':'string'})\n",
    "df_tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_positive = len(df_positive)\n",
    "df_positive['climate_related'] = np.ones(n_positive, dtype=bool)\n",
    "\n",
    "df_negative = df_tweets.sample(n_positive, random_state = 2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keywords = [\"climate action\", \"environment\", \"climate change\", \"renewable\", \"recycle\", \"earth\", \"emission\", \"carbon footprint\", ...]\n",
    "with open(\"keywords.txt\", \"r\") as f:\n",
    "    keywords = [line.rstrip() for line in f]\n",
    "negative = df_negative.text.apply(lambda text: not any([keyword in text.lower() for keyword in keywords]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_negative = df_negative[negative]\n",
    "df_negative['climate_related'] = np.zeros(len(df_negative), dtype=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_positive, df_negative]).sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up(tweet): \n",
    "\n",
    "    parsed_tweet = tweet\n",
    "    \n",
    "    # remove url\n",
    "    parsed_tweet = re.sub(r\"\\S*https?:\\S*\", \"\", parsed_tweet, flags=re.MULTILINE)\n",
    "\n",
    "    # remove non-ascii\n",
    "    parsed_tweet = re.sub(r\"[^\\x00-\\x7F]+\", \"\", parsed_tweet)\n",
    "\n",
    "    # remove 'RT'\n",
    "    parsed_tweet = re.sub(r\"^RT\\s\", \"\", parsed_tweet)\n",
    "    return parsed_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df, embed):\n",
    "    '''\n",
    "    Preprocess the dataframe into spacy pipeline for later classification\n",
    "    ---\n",
    "    Input:\n",
    "    df (DataFrame): Pandas dataframe containing the raw text and outputs.\n",
    "    embed (str): Name of pipeline embedding used\n",
    "\n",
    "    Output:\n",
    "    df (DataFrame): Preprocessed input dataframe\n",
    "    docs (doc): SpaCy doc object that stores text data along with classification\n",
    "    '''\n",
    "\n",
    "    # clean up tweets\n",
    "    df.text = df.text.apply(clean_up)\n",
    "\n",
    "    # Store the data into tuples\n",
    "    data = tuple(zip(df.text.tolist(), df.climate_related.tolist())) \n",
    "    \n",
    "    # Load English library from SpaCy\n",
    "    nlp = spacy.load(embed)\n",
    "    print(data[0])\n",
    "\n",
    "    # Storage for docs\n",
    "    docs = []\n",
    "\n",
    "    # One-hot encoding for the classifications\n",
    "    for doc, label in tqdm(nlp.pipe(data, as_tuples=True), total = len(data)):\n",
    "        \n",
    "        if label:\n",
    "            doc.cats['climate_related'] = 1\n",
    "        else:\n",
    "            doc.cats['climate_related'] = 0\n",
    "        # print(doc.cats)\n",
    "        \n",
    "        docs.append(doc)\n",
    "    return df, docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 24430 Valid: 6108\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, valid_df = train_test_split(df, test_size=0.2)\n",
    "\n",
    "print(\"Train:\",len(train_df), \"Valid:\", len(valid_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the model in installed in current conda env\n",
    "\n",
    "# embed = \"en_core_web_sm\"\n",
    "# embed = \"en_core_web_md\"\n",
    "embed = \"en_core_web_lg\"\n",
    "# embed = \"en_core_web_trf\"\n",
    "\n",
    "assert spacy.util.is_package(embed), embed + \" is not installed. To install, run: `!python -m spacy download \\\"\" + embed + \"\\\"`\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Ever wonder what it takes to build the Cat machines you see on construction sites, or how a race team gets a car ready for peak performance? Watch a behind the scenes video to see how it happens. ', False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24430/24430 [01:12<00:00, 335.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('@Earth8Beauty: #Green beauty ', True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6108/6108 [00:18<00:00, 325.29it/s]\n"
     ]
    }
   ],
   "source": [
    "# Covert the train and valid dataframes to .spacy files for training\n",
    "\n",
    "# Preprocess the dataframes for train data\n",
    "train_data, train_docs = preprocess(train_df, embed)\n",
    "# # Save data and docs in a binary file to disc\n",
    "doc_bin = DocBin(docs=train_docs)\n",
    "doc_bin.to_disk(\"./spacy_data/textcat_train.spacy\")\n",
    "\n",
    "# # Preprocess the dataframes for validation data\n",
    "valid_data, valid_docs = preprocess(valid_df, embed)\n",
    "# # Save data and docs in a binary file to disc\n",
    "doc_bin = DocBin(docs=valid_docs)\n",
    "doc_bin.to_disk(\"./spacy_data/textcat_valid.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Generated config template specific for your use case\u001b[0m\n",
      "- Language: en\n",
      "- Pipeline: textcat_multilabel\n",
      "- Optimize for: accuracy\n",
      "- Hardware: CPU\n",
      "- Transformer: None\n",
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
      "config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy init config \"./config.cfg\" --lang en --pipeline textcat_multilabel --optimize efficiency --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-06-15 19:59:20,174] [DEBUG] Config overrides from CLI: ['paths.train', 'paths.dev']\n",
      "\u001b[38;5;4mℹ Saving to output directory: output\u001b[0m\n",
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[2022-06-15 19:59:20,656] [INFO] Set up nlp object from config\n",
      "[2022-06-15 19:59:20,703] [DEBUG] Loading corpus from path: spacy_data/textcat_valid.spacy\n",
      "[2022-06-15 19:59:20,704] [DEBUG] Loading corpus from path: spacy_data/textcat_train.spacy\n",
      "[2022-06-15 19:59:20,704] [INFO] Pipeline: ['tok2vec', 'textcat_multilabel']\n",
      "[2022-06-15 19:59:20,708] [INFO] Created vocabulary\n",
      "[2022-06-15 19:59:23,260] [INFO] Added vectors: en_core_web_lg\n",
      "[2022-06-15 19:59:25,741] [INFO] Finished initializing nlp object\n",
      "[2022-06-15 19:59:39,763] [INFO] Initialized pipeline components: ['tok2vec', 'textcat_multilabel']\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "[2022-06-15 19:59:39,796] [DEBUG] Loading corpus from path: spacy_data/textcat_valid.spacy\n",
      "[2022-06-15 19:59:39,797] [DEBUG] Loading corpus from path: spacy_data/textcat_train.spacy\n",
      "[2022-06-15 19:59:39,801] [DEBUG] Removed existing output directory: output/model-best\n",
      "[2022-06-15 19:59:39,869] [DEBUG] Removed existing output directory: output/model-last\n",
      "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'textcat_multilabel']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS TEXTC...  CATS_SCORE  SCORE \n",
      "---  ------  ------------  -------------  ----------  ------\n",
      "  0       0          0.23           0.52       41.98    0.42\n",
      "  0     200          4.78          94.50       50.25    0.50\n",
      "  0     400          0.00         100.01       50.22    0.50\n",
      "  0     600          0.00         100.14       50.22    0.50\n",
      "  0     800          0.00          95.65       50.24    0.50\n",
      "  0    1000          0.00          98.99       50.24    0.50\n",
      "  0    1200          0.00          96.35       50.28    0.50\n",
      "  0    1400          0.00          97.38       48.89    0.49\n",
      "  0    1600          0.00          96.89       50.28    0.50\n",
      "  0    1800          0.00          98.09       50.28    0.50\n",
      "  0    2000          0.00          97.08       50.28    0.50\n",
      "  1    2200         68.92          96.33       50.02    0.50\n",
      "  1    2400          0.00          96.58       50.02    0.50\n",
      "  1    2600          0.00         100.26       50.03    0.50\n",
      "  2    2800          3.12         100.27       49.56    0.50\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "output/model-last\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train ./config.cfg --output ./output --paths.train ./spacy_data/textcat_train.spacy --paths.dev ./spacy_data/textcat_valid.spacy --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "79fabbbd37ad3894f6cfb0be2f75783fc59e43226a3dc1366dded6fba97273cc"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('esg-frontier')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
